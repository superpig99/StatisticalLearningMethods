> 感知机模型定位：感知机属于 二分类模型/线性模型/非概率模型/判别模型
> 
> 回顾：统计学习三要素：模型+策略+算法
> 
> 详见博客：https://blog.csdn.net/zyl_wjl_1413/article/details/121733026
## 算法原理
### 模型
- 输入空间/特征空间：$X \subseteq R^n$
- 输出空间：$y \in$ {-1,+1}
- 输入到输出的映射：$y=sgn(wx+b)$ 【sgn为符号函数】
- 假设空间：{f|f(x)=wx+b}

**几何解释**：wx+b=0是特征空间中的一个超平面S，w是该平面的法向量，b是截距；
**前提假设**：当数据集**线性可分**时，感知机才具有可用性；
### 策略
感知机的**损失函数**为误分类的**点x到超平面S的距离**：$\frac{1}{||w||}|wx+b|$ （点到平面的距离公式），但这种含有绝对值的形式并不利于求导，因此，需要想办法去掉绝对值；

对于误分类的点$x_i$而言，满足以下式子：$-y_i(w·x_i+b)>0$，于是，**感知机的损失函数**为：$-\frac{1}{||w||}y_i(wx_i+b)$；

不考虑||w||，于是，就得到了感知机的**风险/目标函数**：$L(w,b)=-\sum_i y_i(wx_i+b)$，注意，这里的风险函数并没有像均方误差那样取平均【模型的目标函数是需要根据模型的特点设定的】

### 算法
感知机采用随机梯度下降算法进行最优解的求解；
#### 原始形式
对L(w,b)求偏导，得到梯度：
$\nabla_wL(w,b)=-\sum_i y_ix_i$
$\nabla_bL(w,b)=-\sum_i y_i$

于是，随机选取一个误分类点xi，w和b的更新如下：【$\eta$为学习率】
$w=w+\eta y_ix_i$
$b=b+\eta y_i$

#### 对偶形式【值得仔细理解】
考虑感知机的参数更新过程，假设共进行了k次更新，$k=\sum_ik_i$，其中，$k_i$为第i个点的更新次数，那么最后得到的w其实等于$w=\sum_{i=1}^m\alpha_i^{k_i}y_ix_i$，其中，$\alpha^{k_i}$ 为对第i个样本点的$k_i$次更新之后的参数；

直观理解就是，对每个样本点的更新体现在$\alpha^{k_i}$上，而所有更新之后的样本点之和就是w。

所以，感知机模型可定义为$y=sgn(\sum_{i=1}^m\alpha_iy_ix_i·x+b)$，这里$\alpha_i$表示模型训练后得到的最优参数

因此，我们可以将对w的更新转换为对$\alpha$的更新，且对误分类点xi而言，参数更新公式为$\alpha_i=\alpha_i+\eta$

**注意：**
- 这里的$\alpha$是m维向量，m为输入样本的个数，也就是，对每个样本，都会有一个相应的参数！
- 直观理解参数$\alpha$的更新：若第i个样本被误分类$n_i$次，则$\alpha_i$就被更新$n_i$次，每次更新，都增加$\eta$，最后，第i个样本对参数的贡献为$w_i=\alpha_ix_iy_i$，将所有样本的参数贡献求和，就得到了最后的w；
- 对偶形式的好处：每次进行参数更新时，无需将样本点纳入计算；

### 算法收敛性——Novikoff定理
暂略
